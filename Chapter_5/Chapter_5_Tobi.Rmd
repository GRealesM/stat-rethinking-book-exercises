---
title: "Chapter_5_Tobi"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
library(tidyverse)
```


## Easy questions

E1) Which of the linear models below are multiple linear regressions?

$$(1) µ_i = α + βx_i \\
(2) µ_i = β_xx_i + β_zz_i \\
(3) µ_i = α + β(x_i − z_i) \\
(4) µ_i = α + β_xxi + β_zz $$ 

Models 2 and 4 are multiple linear regressions

E2) Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition.

$${Ad}_i = \alpha + β_LL_i + β_{Pd}{Pd}_i$$
E3) Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree.
Write down the model definition and indicate which side of zero each slope parameter should be on.


$${Ttd}_i = \alpha + β_FF_i + β_{Ls}{Ls}_i$$
$β_F + β_{Ls}$ should both be greater than 0 as they are positively correlated with time to PhD degree.


E4) Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C and D. Let Ai be an indicator variable that is 1 where case i is in category A. Also suppose Bi, Ci, and Di for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it’s possible to compute one posterior distribution from the posterior distribution of another model.
$$(1) µi = α + βAAi + βBBi + βDDi \\
(2) µi = α + βAAi + βBBi + βCCi + βDDi \\
(3) µi = α + βBBi + βCCi + βDDi \\
(4) µi = αAAi + αBBi + αCCi + αDDi \\
(5) µi = αA(1 − Bi − Ci −Di) + αBBi + αCCi + αDDi$$

## Medium Questions

M1. Invent your own example ofa spurious correlation. An outcome variable should be correlated withbothpredictor variables. But when bothpredictors are entered in the samemodel, the correlation between the outcome and one of the predictors should mostly vanish (or at least be greatly reduced).

An example of this would be:
Outcome variable - Child GCSE results
Predictor 1 - Combined income of parents
Predictor 2 - Amount of organic produce consumed/year


M2. Invent your own example of a masked relationship. An outcome variable should be correlated with both predictor variables, but in opposite directions. And the two predictor variables should be correlated with one another

An example of this would be:
Outcome variable - Amount of money spent on leisure activities
Predictor 1 - Number of hours of overtime worked
Predictor 2 - Take-home pay


M3. It is sometimes observed that the best predictor of fire risk is the presence of firefighters— States and localities with many firefighters also have more fires. Presumably firefighters do not cause fires. Nevertheless, this is not a spurious correlation. Instead fires cause firefighters. Consider the same reversal of causal inference in the context of the divorce and marriage data. How might a high divorce rate cause a higher marriage rate? Can you think of a way to evaluate this relationship, using multiple regression?

A high divorce rate could cause a high marriage if a culture placed high social value on being married. Ergo getting divorced meant you were likely to remarry rather than remain single.
We can evaluate this by...


5M4. In the divorce data, States with high numbers of members of the Church of Jesus Christ of Latter-day Saints (LDS) have much lower divorce rates than the regression models expected. Find a list of LDS population by State and use those numbers as a predictor variable, predicting divorce rate using marriage rate, median age at marriage, and percent LDS population (possibly standardized).
You may want to consider transformations of the raw percent LDS variable.

```{r M4}
# Load and wrangle data
data("WaffleDivorce")
mormons.df <- read_csv('mormons.csv')

## Results in 49 states with no NAs as Nevada is missing from WaffleDivorce and DC missing from mormons.csv
m4.divorce.df <- left_join(WaffleDivorce, mormons.df, by=c('Location'='State')) %>% 
  mutate(mormonPopPct = mormonPop/Pop * 100) %>% 
  drop_na()

d <- list()
d$A <- standardize( m4.divorce.df$MedianAgeMarriage )
d$D <- standardize( m4.divorce.df$Divorce )
d$M <- standardize( m4.divorce.df$Marriage )
d$L <- standardize( m4.divorce.df$mormonPopPct )

# Write out model
model.m4 <- quap( alist( D ~ dnorm( mu , sigma ) ,
                     mu <- a + bM*M + bA*A + bL*L ,
                     a ~ dnorm( 0 , 0.2 ) ,
                     bM ~ dnorm( 0 , 0.5 ) ,
                     bA ~ dnorm( 0 , 0.5 ) ,
                     bL ~ dnorm( 0 , 1 ) ,
                     sigma ~ dexp( 1 )
                     ) , data = d )

# Fit model
precis(model.m4)

# Look at results
plot( coeftab(model.m4), par=c("bA","bM", "bL") )
```



# Hard questions

H1) In the divorce example, suppose the DAG is: M→ A → D. What are the implied conditional independencies of the graph? Are the data consistent with it?

D is conditionally independent of M, given A
